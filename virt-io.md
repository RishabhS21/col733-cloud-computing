# IO virtualization
Let us review the relevant backgroud on how an OS does IO on x86. 

## Background
Devices either support port-based IO (PIO) or memory-mapped IO (MMIO). For PIO,
x86 has special `in` and `out` instructions to read and write data respectively
from/to ports.  For example, the PS/2 keyboard/mice driver talks to the device
on ports 0x6000-0x6004. 

For MMIO, the devices are mapped into the physical memory. To talk to MMIO
devices, programs just run the regular load/store memory instructions. For
example, physical memory addresses 0xA000-0xA7FF are mapped to the video
controller hardware; 0x9000-0x90FF is mapped to sound controller.  Load/stores
generated by the CPU within the range of devices are forwarded to the
appropriate device by the hardware's address decoding circuitry. Device
specifications declare the meaning of different memory addresses. For example,
the following shows the specification for the e1000 network card. Each device
register is an offset from Base Address Register (e.g, BAR=0xF0000):

| Name   | BAR Offset | Purpose                                                           |
| ------ | ---------- | ----------------------------------------------------------------- |
| STATUS | 0x00008    | Read to find if device is ready                                   |
| ICR    | 0x000C0    | Interrupt Cause Read: read to find why device generated interrupt |
| IMS    | 0x000D0    | Interrupt Mask Set: write to enable interrupts                    |
| IMC    | 0x000D8    | Interrupt Mask Clear: write to disable interrupts                 |

Note that the MMIO addresses are *physical addresses*. Processes typically
cannot directly talk to devices: (1) for PIO, `in` and `out` are ring-0
instructions, and (2) the OS does not map device addresses into process' virtual
address space.

Devices typically do direct-memory access (DMA) to free up CPU from copying data
between DRAM and device. For example, to transmit a lot of network data, the
device driver can set up a ring of pointers pointing to buffers that need to be
sent. This ring is told to the device, and the driver keeps adding buffers to
the ring. If there are buffers to be sent, the device can directly reads buffers
from DRAM to send them over the network. For example, again for the e1000
network card, the following shows the Tx ring management:

| Name  | BAR Offset | Purpose                                                                 |
| ----- | ---------- | ----------------------------------------------------------------------- |
| TDBAH | 0x03800    | Base address of the Tx ring. Initialized by the OS.                     |
| TDLEN | 0x03808    | Tx ring size. Initialized by the OS                                     |
| TDH   | 0x03810    | Pointer to head of ring. Updated by the device after consuming a buffer |
| TDT   | 0x03818    | Pointer to tail of ring. Updated by the OS after adding a new buffer    |

If the ring is full, the driver might enable device interrupts and get an
interrupt when there is more space in the ring. Interrupts are handled via IDT
as [discussed here](./virt-cpu.md).

## IO emulation (Full virtualization)
IO devices can be emulated by trap-and-emulate hypervisors. Since OS is no
longer running in ring-0 of root-mode, its port IO instructions (`in` and `out`)
will trap into VMM. Similarly, VMM can setup the GPA->HPA page table (e.g., mark
corresponding PTEs as not present or read-only) such that when the OS is trying
to do MMIO, it generates a GPA which again traps into the VMM.

KVM forwards these traps to user-level programs, such as QEMU, to emulate the
device. For example, if the Guest was trying read e1000 device ICR, QEMU
might respond in a manner identical to how e1000 NIC responds: e1000 responds
with ICR and clears ICR to zero.

To actually send/receive data over the network, QEMU emulated device would make
regular network system calls into the Host OS. The advantage of emulation is
that the actual hardware may have a completely different NIC; the Guest OS need
not know, nor have drivers for the real NIC.

However, IO emulation can cause too many VM exits. This is because the physical
devices were not designed with virtualization in mind. For example, to send one
packet, e1000 drivers need to read the status twice. But even though status is
read-only, its page has to be mapped as not present. This is because status and
ICR are on the same page. We cannot allow reading ICR without trapping into VMM
since reading ICR also clears ICR! We need to uphold the illusion that the OS is
talking to the real e1000 device.

## IO paravirtualization

To reduce unnecessary VM exits, IO paravirtualization breaks the transparency;
Guest OS knows that it is talking to a "virtual device". Luckily, this does not
require changes in all over the OS; rather just a device driver for the virtual
device needs to be added into the OS. This virtual device interface is designed
carefully to minimize VM exits under the virtio standard.

virtio-net device exposes Rx/Tx rings (called virtqueues) similar to e1000's
Rx/Tx rings. When e1000 driver was adding one buffer to the Tx ring, it had to
undergo 4 VM exits: read STATUS, read TDT, write TDH, again read STATUS.
virtio-net driver can add a bunch of buffers and finally do an explicit exit
"virtio-kick" to ask VMM to transmit. VMM can tell driver "I am already
transmitting, don't kick me again" using NO_NOTIFY. Guest can tell VMM "I have 
many empty slots in the ring, don't interrupt me after you have sent a buffer"
using NO_INTERRUPT.

virtio-net device was shown to provide 22x higher throughput than e1000 NIC on a
NetPerf benchmark.

## Hardware-assisted IO virtualization
Modern devices provide advanced capabilities to remove hypervisor altogether
from the guest VM<>device path. VMMs can ask modern single-root IO
virtualization (SRIOV) devices to create "virtual devices" on demand, also
called virtual functions (VFs); VMMs then directly assigns a VF to a guest VM.
Removing hypervisor from data exchange between Guest VM and the device was shown
to provide 1.31x to 1.78x higher throughput than the virtio-net device.

Physical device presents an isolated view of itself to the VM. However, with 
hypervisor out of the data transfer path, guest can only provide GPAs to the
virtual devices for doing DMA. x86 VT-d extensions add IOMMU; virtual devices
use IOMMU to translate GPAs to HPAs for DMA.